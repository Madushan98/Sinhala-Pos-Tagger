{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ඊශ්‍රායල් NNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f = open('sinhala_pos.txt', 'r')\n",
    "f1 = open('pos_nod.csv', 'a+')\n",
    "writer = csv.writer(f1)\n",
    "\n",
    "lines = tuple(open('sinhala_pos.txt', 'r'))\n",
    "lines = list(lines)\n",
    "lines.pop(len(lines)-1)\n",
    "print(lines[0])\n",
    "words = []\n",
    "tags = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, line in enumerate(lines):\n",
    "    text = line.split()\n",
    "    if(len(text) >= 2):\n",
    "        temp = text\n",
    "        if temp[0] not in words:\n",
    "            words.append(temp[0])\n",
    "            tags.append(temp[1])\n",
    "            writer.writerow(temp)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33108\n",
      "33108\n",
      "418\n",
      "474\n",
      "NUM\n"
     ]
    }
   ],
   "source": [
    "f2 = open('sinhala questions and answers.csv', 'r')\n",
    "reader = csv.reader(f2)\n",
    "\n",
    "senetences = []\n",
    "\n",
    "print(len(words))\n",
    "print(len(tags))\n",
    "\n",
    "for row in reader:\n",
    "    for s in row:\n",
    "        senetences.append(s)\n",
    "\n",
    "nf = []\n",
    "has = []\n",
    "words2 = []\n",
    "\n",
    "for s in senetences:\n",
    "    w = s.split()\n",
    "    for word in w:\n",
    "        if word in words:\n",
    "            has.append(word)\n",
    "        else:\n",
    "            nf.append(word)\n",
    "\n",
    "nf = [*set(nf)]\n",
    "has = [*set(has)]\n",
    "words2 = [*set(words)]\n",
    "print(len(nf))\n",
    "print(len(has))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence = []\n",
    "\n",
    "word = \"නම\"\n",
    "\n",
    "if word in words:\n",
    "    print(tags[words.index(word)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "[('මට', 'PRP'), ('වෙලාව', 'NNC'), ('කියන්න', 'VNN'), ('පුළුවන්ද', 'NIP')]\n"
     ]
    }
   ],
   "source": [
    "all_sent = []\n",
    "no_sent = []\n",
    "for st in senetences:\n",
    "    sent = st.split()\n",
    "    sentence = []\n",
    "    one_left = False\n",
    "    for s in sent:\n",
    "        if s in words:\n",
    "            word = (s, tags[words.index(s)])\n",
    "            sentence.append(word)\n",
    "        if s not in words:\n",
    "            one_left = True\n",
    "            no_sent.append(s)\n",
    "    if one_left == False:\n",
    "        all_sent.append(sentence)\n",
    "\n",
    "# for s in no_sent:\n",
    "#     print(s)\n",
    "\n",
    "\n",
    "\n",
    "print(len(all_sent))\n",
    "print(all_sent[12])\n",
    "\n",
    "# f5 = open('poss_sentence.csv', 'a+')\n",
    "# writer = csv.writer(f5)\n",
    "\n",
    "# for s in all_sent:\n",
    "#     writer.writerow(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pos_tagger():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unknown_prob = 0.0000000000001\n",
    "        self.tagged_file = glob.glob(\"brown/*\")\n",
    "        self.bigram_cnt = {}\n",
    "        self.unigram_cnt = {}\n",
    "        self.tag_count = defaultdict(lambda: 0)\n",
    "        self.tag_word_count = Counter()\n",
    "        self.transition_probabilities = defaultdict(lambda: self.unknown_prob)\n",
    "        self.emmission_probabilities = defaultdict(lambda: self.unknown_prob)\n",
    "\n",
    "    def ngrams(self, text, n):\n",
    "        Ngrams = []\n",
    "        for i in range(len(text)): Ngrams.append(tuple(text[i: i + n]))\n",
    "        return Ngrams\n",
    "\n",
    "    def bigram_counts(self, tags):\n",
    "        for i_tag_bigram in self.ngrams(tags, 2):\n",
    "            if i_tag_bigram in self.bigram_cnt:\n",
    "                self.bigram_cnt[i_tag_bigram] += 1\n",
    "            else:\n",
    "                self.bigram_cnt[i_tag_bigram] = 1\n",
    "        return self.bigram_cnt\n",
    "\n",
    "    def unigram_counts(self, tags):\n",
    "        for tag in tags:\n",
    "            if tag in self.unigram_cnt:\n",
    "                self.unigram_cnt[tag] += 1\n",
    "            else:\n",
    "                self.unigram_cnt[tag] = 1\n",
    "        return self.unigram_cnt\n",
    "\n",
    "    def tag_word_counts(self, tagged_words):\n",
    "        for tag, word in tagged_words:\n",
    "            self.tag_count[tag] += 1\n",
    "            if (word, tag) in self.tag_word_count:\n",
    "                self.tag_word_count[(tag, word)] += 1\n",
    "            else:\n",
    "                self.tag_word_count[(tag, word)] = 1\n",
    "        return self.tag_word_count\n",
    "\n",
    "    def transition_probabilty(self, tags):\n",
    "        bigrams = self.ngrams(tags, 2)\n",
    "        for bigram in bigrams:\n",
    "            self.transition_probabilities[bigram] = self.bigram_cnt[bigram] / self.unigram_cnt[bigram[0]]\n",
    "        return self.transition_probabilities\n",
    "\n",
    "    def emmission_probabilty(self, tagged_words):\n",
    "        for tag, word in tagged_words:\n",
    "            self.emmission_probabilities[tag, word] = self.tag_word_count[tag, word] / self.tag_count[tag]\n",
    "        return self.emmission_probabilities\n",
    "\n",
    "    def initial_probabilities(self, tag):\n",
    "        return self.transition_probabilities[\"START\", tag]\n",
    "\n",
    "    def vertibi(self, observable, in_states):\n",
    "        states = set(in_states)\n",
    "        states.remove(\"START\")\n",
    "        states.remove(\"END\")\n",
    "        trails = {}\n",
    "        for s in states:\n",
    "            trails[s, 0] = self.initial_probabilities(s) * self.emmission_probabilities[s, observable[0]]\n",
    "        for o in range(1, len(observable)):\n",
    "            obs = observable[o]\n",
    "            for s in states:\n",
    "                v1 = [(trails[k, o - 1] * self.transition_probabilities[k, s] * self.emmission_probabilities[s, obs], k) for k in states]\n",
    "                k = sorted(v1)[-1][1]\n",
    "                trails[s, o] = trails[k, o - 1] * self.transition_probabilities[k, s] * self.emmission_probabilities[s, obs]\n",
    "        best_path = []\n",
    "        for o in range(len(observable) - 1, -1, -1):\n",
    "            k = sorted([(trails[k, o], k) for k in states])[-1][1]\n",
    "            best_path.append((observable[o], k))\n",
    "        best_path.reverse()\n",
    "        for x in best_path:\n",
    "            print(str(x[0]) + \",\" + str(x[1]))\n",
    "        return best_path\n",
    "\n",
    "    def clean(self, word):\n",
    "        word = re.sub('\\s+', '', word.lower())\n",
    "        return word\n",
    "\n",
    "    def tag_test(self, all_tags):\n",
    "        words = []\n",
    "        with open(\"tag_test.txt\") as f:\n",
    "            for line in f:\n",
    "                if \"sentence ID\" in line:\n",
    "                    words = []\n",
    "                    print(line)\n",
    "                elif \"<EOS>\" in line:\n",
    "                    self.vertibi([self.clean(w) for w in words], all_tags)\n",
    "                    print(\"<EOS>\")\n",
    "                else:\n",
    "                    words.append(line)\n",
    "\n",
    "    def tag(self):\n",
    "        reader_corpus = TaggedCorpusReader('.',\n",
    "                                           self.tagged_file)\n",
    "\n",
    "        tagged_words = []\n",
    "        all_tags = []\n",
    "        for sent in reader_corpus.tagged_sents():  # get tagged sentences\n",
    "            all_tags.append(\"START\")\n",
    "            for (word, tag) in sent:\n",
    "                if tag is None or tag in ['NIL']:\n",
    "                    continue\n",
    "                all_tags.append(tag)\n",
    "                word = self.clean(word)\n",
    "                tagged_words.append((tag, word))\n",
    "            all_tags.append(\"END\")\n",
    "\n",
    "        self.tag_word_counts(tagged_words)\n",
    "\n",
    "        self.bigram_cnt = self.bigram_counts(all_tags)\n",
    "        self.unigram_cnt = self.unigram_counts(all_tags)\n",
    "\n",
    "        self.transition_probabilty(all_tags)\n",
    "        self.emmission_probabilty(tagged_words)\n",
    "\n",
    "        self.tag_test(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = pos_tagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = [\"We\",\n",
    "            \"have\",\n",
    "            \"learned\",\n",
    "            \"much\",\n",
    "            \"about\",\n",
    "            \"interstellar\",\n",
    "            \"drives\",\n",
    "            \"since\",\n",
    "            \"a\",\n",
    "            \"hundred\",\n",
    "            \"years\",\n",
    "            \"ago\",\n",
    "            \"that\",\n",
    "            \"is\",\n",
    "            \"all\",\n",
    "            \"I\",\n",
    "            \"can\",\n",
    "            \"tell\",\n",
    "            \"you\",\n",
    "            \"about\",\n",
    "            \"them\",\n",
    "            ]\n",
    "all_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_sent = [ps.clean(w) for w in test_sent]\n",
    "print(ps.vertibi(cleaned_test_sent, all_tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0380d3a852a1b96b28c7a2f1c467f980e4f31b019e30308669f763d5d39a2a5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
